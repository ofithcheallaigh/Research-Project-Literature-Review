@online{statista1,
    author = "Statista Research Department",
    title = "Internet of Things (IoT) connected devices installed base worldwide from 2015 to 2025",
    year = 2016,
    url  = "https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/",
    addendum = "(accessed: 22.10.2022)",
    keywords = "IoT"
}

@article{Cao2020,
   abstract = {With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today's intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.},
   author = {Keyan Cao and Yefan Liu and Gongjie Meng and Qimeng Sun},
   doi = {10.1109/ACCESS.2020.2991734},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Edge computing,Internet of Things,cloud computing},
   pages = {85714-85728},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Overview on Edge Computing Research},
   volume = {8},
   year = {2020},
}

@inbook{pythonML,
    title = {Python Machine Learning - Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2},
    author = {Sebastian Raschka, Vahid Mirjalili},
    year = {2019},
    publisher = {Packt Publishing},
    chapter = "1",
    keywords = {Python, Machine Learning}
}

@online{google1,
    author = "Google Development Team",
    title = "What Is Clustering",
    url  = "https://developers.google.com/machine-learning/clustering/overview",
    addendum = "(accessed: 26.10.2022)",
    keywords = "Clustering"
}

@online{azure,
    author = {{Lauryn Gayhardt, et. al.}},
    title = "Machine Learning Algorithm Cheat Sheet for Azure Machine Learning designer",
    url  = "https://learn.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet",
    addendum = "(accessed: 26.10.2022)",
    keywords = "Algotithms"
}

@inbook{pythonDL,
    title = {Deep Learning with Python},
    author = {Fran√ßois Chollet},
    year = {2021},
    publisher = {Manning Publishions Co.},
    chapter = "1",
    keywords = {Python, Deep Learning}
}

@online{mccarthy1,
    author = "Unknown",
    title = "Professor John McCarthy - General Information",
    url  = "http://jmc.stanford.edu/general/index.html",
    addendum = "(accessed: 29.10.2022)",
    keywords = "McCarthy, AI"
}

@report{whatisai_mccarthy,
   abstract = {This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI.},
   author = {John Mccarthy},
   title = {WHAT IS ARTIFICIAL INTELLIGENCE?},
   url = {http://www-formal.stanford.edu/jmc/},
   year = {2007},
}

@online{ibm_ml,
   author = {{IBM Cloud Education}},
   title = {Machine Learning},
   year = 2020,
   url = {https://www.ibm.com/cloud/learn/machine-learning},
   addendum = "(accessed: 19.11.2022)",
}

@article{Nicholas_Lane_2017,
   abstract = {This work observes that a large fraction of the computations performed by Deep Neural Networks (DNNs) are intrinsically ineffectual as they involve a multiplication where one of the inputs is zero. This observation motivates Convolution (CNV), a value-based approach to hardware acceleration that eliminates most of these ineffectual operations, improving performance and energy over a state-of-the-art accelerator with no accuracy loss. },
   author = {Nicholas D Lane and Sourav Bhattacharya and Akhil Mathur and Petko Georgiev and Claudio Forlivesi and Fahim Kawsar},
   doi = {10.1109/MPRV.2017.2940968},
   journal = {Prevasive Computing},
   month = {7},
   pages = {82-88},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Squeezing Deep Learning into Mobile and Embedded Devices},
   url = {https://ieeexplore.ieee.org/abstract/document/7994570},
   year = {2017},
}

@inproceedings{han_s_2017,
   abstract = {Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built increasingly larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption and leads to a high total cost of ownership (TCO) of a data center. To speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy.},
   author = {Song Han and Junlong Kang and Huizi Mao and Yiming Hu and Xin Li and Yubin Li and Dongliang Xie and Hong Luo and Song Yao and Yu Wang and Huazhong Yang and William J. Dally},
   doi = {10.1145/3020078.3021745},
   isbn = {9781450343541},
   journal = {FPGA 2017 - Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
   keywords = {Deep learning,FPGA,Hardware acceleration,Model compression,Software-hardware co-design,Speech recognition},
   month = {2},
   pages = {75-84},
   publisher = {Association for Computing Machinery, Inc},
   title = {ESE: Efficient speech recognition engine with sparse LSTM on FPGA},
   url = {https://arxiv.org/pdf/1612.00694.pdf},
   year = {2017},
}

@inproceedings{Likamwa2016,
   abstract = {Continuous mobile vision is limited by the inability to efficiently capture image frames and process vision features. This is largely due to the energy burden of analog readout circuitry, data traffic, and intensive computation. To promote efficiency, we shift early vision processing into the analog domain. This results in RedEye, an analog convolutional image sensor that performs layers of a convolutional neural network in the analog domain before quantization. We design RedEye to mitigate analog design complexity, using a modular column-parallel design to promote physical design reuse and algorithmic cyclic reuse. RedEye uses programmable mechanisms to admit noise for tunable energy reduction. Compared to conventional systems, RedEye reports an 85\% reduction in sensor energy, 73\% reduction in cloudlet-based system energy, and a 45\% reduction in computation-based system energy.},
   author = {Robert Likamwa and Yunhui Hou and Yuan Gao and Mia Polansky and Lin Zhong},
   doi = {10.1109/ISCA.2016.31},
   isbn = {9781467389471},
   journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
   keywords = {computer vision,continuous mobile vision,pre-quantization processing,programmable analog computing},
   month = {8},
   pages = {255-266},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {RedEye: Analog ConvNet Image Sensor Architecture for Continuous Mobile Vision},
   year = {2016},
}

@inproceedings{Lane2016,
   abstract = {Breakthroughs from the field of deep learning are radically changing how sensor data are interpreted to extract the high-level information needed by mobile apps. It is critical that the gains in inference accuracy that deep models afford become embedded in future generations of mobile apps. In this work, we present the design and implementation of DeepX, a software accelerator for deep learning execution. DeepX signif- icantly lowers the device resources (viz. memory, computation, energy) required by deep learning that currently act as a severe bottleneck to mobile adoption. The foundation of DeepX is a pair of resource control algorithms, designed for the inference stage of deep learning, that: (1) decompose monolithic deep model network architectures into unit- blocks of various types, that are then more efficiently executed by heterogeneous local device processors (e.g., GPUs, CPUs); and (2), perform principled resource scaling that adjusts the architecture of deep models to shape the overhead each unit-blocks introduces. Experiments show, DeepX can allow even large-scale deep learning models to execute efficently on modern mobile processors and significantly outperform existing solutions, such as cloud-based offloading.},
   author = {Nicholas D. Lane and Sourav Bhattacharya and Petko Georgiev and Claudio Forlivesi and Lei Jiao and Lorena Qendro and Fahim Kawsar},
   doi = {10.1109/IPSN.2016.7460664},
   isbn = {9781509008025},
   journal = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks, IPSN 2016 - Proceedings},
   month = {4},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices},
   year = {2016},
}
@article{Gong2014,
   abstract = {Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classification accuracy using the state-of-the-art CNN.},
   author = {Yunchao Gong and Liu Liu and Ming Yang and Lubomir Bourdev},
   month = {12},
   title = {Compressing Deep Convolutional Networks using Vector Quantization},
   url = {http://arxiv.org/abs/1412.6115},
   year = {2014},
}

